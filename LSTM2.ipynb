{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ramon284/MachineLearningVu/blob/main/LSTM2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDcihPm-hg-6",
        "outputId": "d0337fd9-dc90-400e-c307-db2d4c7e21a4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "618NlGa5le6Z"
      },
      "source": [
        "#Load values created by data preprocessing\n",
        "\n",
        "\n",
        "max_sequence_len = 90 #Hardcoded, taken from data build script\n",
        "total_words = 6215  #Hardcoded, taken from data build script"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VD4AGz9duBuz"
      },
      "source": [
        "import numpy\n",
        "\n",
        "labeltemp = numpy.load(\"/content/drive/MyDrive/all_labels.npy\")\n",
        "\n",
        "labels = [labeltemp]"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MR0qv-z1X17A"
      },
      "source": [
        "predictors = numpy.load(\"/content/drive/MyDrive/predictors.npy\")\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "be_Jmw3xuXty",
        "outputId": "b76d338b-d566-46c7-a004-7aae128a75de"
      },
      "source": [
        "print(labels)\n",
        "print(predictors)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[array([[0., 0., 0., ..., 0., 0., 0.],\n",
            "       [0., 0., 0., ..., 0., 0., 0.],\n",
            "       [0., 0., 0., ..., 0., 0., 0.],\n",
            "       ...,\n",
            "       [0., 0., 0., ..., 0., 0., 0.],\n",
            "       [0., 0., 0., ..., 0., 0., 0.],\n",
            "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)]\n",
            "[[  0   0   0 ...   0   0   8]\n",
            " [  0   0   0 ...   0   8   9]\n",
            " [  0   0   0 ...   8   9  36]\n",
            " ...\n",
            " [  0   0   0 ... 643  13  27]\n",
            " [  0   0   0 ...  13  27  40]\n",
            " [  0   0   0 ...  27  40 323]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSZz5lDqZ19o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "300bbbe8-88b4-47ad-9351-38e6c425e474"
      },
      "source": [
        "#Check what gpu/tpu we are running on\n",
        "!nvidia-smi"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Mar  4 14:05:42 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.39       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   72C    P0    72W / 149W |    189MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YW2xxm9LvY7K"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.models import Sequential\n",
        "import keras.utils as ku "
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LO_NK-lj-HM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4e29e93-da02-49f1-f736-d2ce098204c1"
      },
      "source": [
        "\n",
        "# #4. Model\n",
        "def create_model(max_sequence_len, total_words):\n",
        "    input_len = max_sequence_len - 1\n",
        "    model = Sequential()\n",
        "    \n",
        "    # Add Input Embedding Layer\n",
        "    model.add(Embedding(total_words, 10, input_length=input_len))\n",
        "    \n",
        "    # Add Hidden Layer 1 - LSTM Layer\n",
        "    model.add(LSTM(100))\n",
        "    model.add(Dropout(0.1))\n",
        "    \n",
        "    # Add Output Layer\n",
        "    model.add(Dense(total_words, activation='softmax'))\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "    \n",
        "    return model\n",
        "\n",
        "model = create_model(max_sequence_len, total_words)\n",
        "model.summary()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 89, 10)            62150     \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 100)               44400     \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 6215)              627715    \n",
            "=================================================================\n",
            "Total params: 734,265\n",
            "Trainable params: 734,265\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V93Ft8MDXWMd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f657d3d-d43c-4e44-8731-3bacf048fac5"
      },
      "source": [
        "#Train the Model\n",
        "model.fit(predictors, labels, epochs=20, verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "1563/1563 [==============================] - 35s 22ms/step - loss: 6.5771\n",
            "Epoch 2/20\n",
            "1563/1563 [==============================] - 35s 22ms/step - loss: 6.2285\n",
            "Epoch 3/20\n",
            "1563/1563 [==============================] - 35s 22ms/step - loss: 6.0055\n",
            "Epoch 4/20\n",
            "1563/1563 [==============================] - 35s 22ms/step - loss: 5.7876\n",
            "Epoch 5/20\n",
            " 808/1563 [==============>...............] - ETA: 17s - loss: 5.5733"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bvUrF-SXYyt"
      },
      "source": [
        "# # #5. Generating texts\n",
        "def generate_text(seed_text, next_words, model, max_sequence_len):\n",
        "    for _ in range(next_words):\n",
        "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "        predicted = model.predict_classes(token_list, verbose=0)\n",
        "        \n",
        "        output_word = \"\"\n",
        "        for word,index in tokenizer.word_index.items():\n",
        "            if index == predicted:\n",
        "                output_word = word\n",
        "                break\n",
        "        seed_text += \" \"+output_word\n",
        "    return seed_text.title()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}