{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "import keras.utils as ku\n",
    "from keras.callbacks import EarlyStopping\n",
    "#TF_FORCE_GPU_ALLOW_GROWTH=True\n",
    "#import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Unique Quotes: (36197,)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('quotesFiltered.csv', sep=';')\n",
    "data = data.drop(data[data.QUOTE.str.count(\"\\.\") > 1].index) ## remove quotes with more than 1 sentence by counting dots\n",
    "data = data['QUOTE'].str.lower() ##makes all strings lowercase\n",
    "quotes = data.drop_duplicates()\n",
    "print(f\"Total Unique Quotes: {quotes.shape}\")\n",
    "\n",
    "\n",
    "all_quotes = list(quotes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique words in the text corpus: 24212\n",
      "81\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "\n",
    "def generate_sequences(corpus):\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    total_words = len(tokenizer.word_index) + 1\n",
    "    print(f\"Total unique words in the text corpus: {total_words}\")\n",
    "    input_sequences = []\n",
    "    for line in corpus:\n",
    "        seq = tokenizer.texts_to_sequences([line])[0]\n",
    "        for i in range(1, len(seq)):\n",
    "            ngram_seq = seq[:i+1]\n",
    "            input_sequences.append(ngram_seq)\n",
    "            \n",
    "    return input_sequences, total_words\n",
    "\n",
    "# Generating sequences\n",
    "input_sequences, total_words = generate_sequences(all_quotes)\n",
    "input_sequences[:5]\n",
    "maxlen = max([len(x) for x in input_sequences])\n",
    "print(maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating predictors and labels from the padded sequences\n",
    "#def generate_input_sequence(input_sequences):\n",
    "#    ##maxlen = max([len(x) for x in input_sequences])\n",
    "#    input_sequences = pad_sequences(input_sequences, maxlen=maxlen)\n",
    "#    predictors, label = input_sequences[:, :-1], input_sequences[:, -1]\n",
    "#    label = ku.to_categorical(label, num_classes=total_words)\n",
    "#    return predictors, label  ##, maxlen\n",
    "#\n",
    "#predictors, label = generate_input_sequence(input_sequences)\n",
    "#predictors[:1], label[:1]\n",
    "#print(maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create the model\n",
    "#embedding_dim = 64\n",
    "#\n",
    "#def create_model(maxlen, embedding_dim, total_words):\n",
    "#    model = Sequential()\n",
    "#    model.add(layers.Embedding(total_words, embedding_dim, input_length = maxlen,mask_zero=False,))\n",
    "#    model.add(layers.LSTM(64, dropout=0.2))\n",
    "#    model.add(layers.Dense(total_words, activation='softmax'))\n",
    "#   \n",
    "#    # compiling the model\n",
    "#    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "#    return model\n",
    "#\n",
    "#model = create_model(maxlen, embedding_dim, total_words)\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train the model\n",
    "#model.fit(predictors, label, epochs=50, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model for later use\n",
    "#model.save(\"Quotes_generator.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 80, 64)            1549568   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 24212)             1573780   \n",
      "=================================================================\n",
      "Total params: 3,156,372\n",
      "Trainable params: 3,156,372\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model1 = load_model(\"Quotes_generator_low_training.h5\") ## Trained 50 epochs on 10% of the dataset\n",
    "model2 = load_model(\"Quotes_generator_high_training.h5\") ## Trained 25 epochs on the full dataset in chunks of 10\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_quote(seed_text, num_words, model, maxlen):\n",
    "    \n",
    "    for _ in range(num_words):\n",
    "        seed_text = seed_text.lower()\n",
    "        tokens = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        tokens = pad_sequences([tokens], maxlen=maxlen, padding='pre')\n",
    "        \n",
    "        predicted = model.predict_classes(tokens)\n",
    "        \n",
    "        output_word = ''\n",
    "        \n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted:\n",
    "                output_word = word\n",
    "                break\n",
    "        seed_text = seed_text + \" \" + output_word\n",
    "    \n",
    "    return seed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "\n",
    "\n",
    "wordCategories = ['wordLists/loveWords.txt', 'wordLists/politicsWords.txt', 'wordLists/ageWords.txt'] ## contains words related to a certain topic\n",
    "\n",
    "with open('wordLists/illegalWordsList.txt') as f:\n",
    "    temp = f.readlines()\n",
    "illegalWords = []\n",
    "for element in temp:\n",
    "    illegalWords.append(element.strip())\n",
    "\n",
    "\n",
    "def chooseStartingWord(wordCategories): ## chooses a random word from a chosen topic.\n",
    "    temp = random.uniform(0, 1)\n",
    "    pronouns = ''\n",
    "    \n",
    "    if(temp < 0.8): ## 80% chance to start the sentence with a random word like for example \"war\"\n",
    "        pronoun = '' ## 20% chance to start the sentence with some extra words, like \"The war\" or \"in case of war\"\n",
    "    else:\n",
    "        with open('wordLists/startingWords.txt') as f:\n",
    "            pronouns = f.readlines()\n",
    "        pronoun = random.choice(pronouns)\n",
    "    \n",
    "    with open(random.choice(wordCategories)) as f: ## picks a random category\n",
    "        words = f.readlines() \n",
    "    random_word = pronoun + random.choice(words) ## picks a random word from that category\n",
    "    return random_word\n",
    "\n",
    "def removeDuplicates(sentence):\n",
    "    chars = list(sentence) ## if 2 duplicate words are next to each other, remove 1 of them\n",
    "    prev = None            ## so \"there is is is a fire fire\" --> \"there is a fire\"\n",
    "    k = 0\n",
    "    for c in sentence:\n",
    "        if prev != c:\n",
    "            chars[k] = c\n",
    "            prev = c\n",
    "            k = k + 1\n",
    "    return ' '.join(chars[:k])\n",
    "\n",
    "def filterQuote(quote, wordList):\n",
    "    x = True\n",
    "    words = quote.split() ## turn quote string into an array of words\n",
    "    while x == True:\n",
    "        if words[-1] in wordList: ## check if last word of sentence is legal or not\n",
    "            words = words[:-1]    ## if illegal, simply remove it.\n",
    "        else:\n",
    "            x = False\n",
    "            \n",
    "    words = removeDuplicates(words) ## filter duplicates\n",
    "    filteredQuote = words.capitalize() + '.' ## capitalization and punctuation\n",
    "    filteredQuote = re.sub(r'\\bi\\b', 'I', filteredQuote) ## regex baby, turns \"i\" into \"I\"\n",
    "    return filteredQuote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 :  An age in a world effective if advertising basically I the star movies as pain as one questions if a fact need.\n",
      "Model 2 :  An age of wisdom is the beginning of wisdom and wisdom is to be humble and the truth that is the only.\n"
     ]
    }
   ],
   "source": [
    "startWord = 'an age'\n",
    "length = 20\n",
    "## not sure wether maxlen should be equal to number of words, or be longer. it was trained on maxlen ~= 80\n",
    "\n",
    "outputQuote = generate_quote(startWord, num_words = length, model= model1, maxlen=length)\n",
    "filteredQuote = filterQuote(outputQuote, illegalWordsList)\n",
    "print(\"Model 1 : \",filteredQuote)\n",
    "outputQuote2 = generate_quote(startWord, num_words = length, model= model2, maxlen=length)\n",
    "filteredQuote2 = filterQuote(outputQuote2, illegalWordsList)\n",
    "print(\"Model 2 : \",filteredQuote2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomly chosen starting word:  while\n",
      "eons\n",
      "\n",
      "Model 1 :  While eons not a world creating and a family I technology of the wrestling well one environment of me seen knowledge not men.\n",
      "Model 2 :  While eons the technology is the hardest and balance is a blessing however we can be a leader in our economy culture and.\n",
      "------------------------------------------------------------------------------------------------------\n",
      "Randomly chosen starting word:  inequality\n",
      "\n",
      "Model 1 :  Inequality your wedding I the us of the humanity all but men 'if chaos of I.\n",
      "Model 2 :  Inequality was the hardest work in the world and the world in the world and the world.\n",
      "------------------------------------------------------------------------------------------------------\n",
      "Randomly chosen starting word:  don't forget that\n",
      "wisdom\n",
      "\n",
      "Model 1 :  Don't forget that wisdom loved in a centuries treated and imposed or and the battle and the stupid in that in that as.\n",
      "Model 2 :  Don't forget that wisdom is to be a teacher to be a teacher to be a teacher to be able to travel and.\n",
      "------------------------------------------------------------------------------------------------------\n",
      "Randomly chosen starting word:  wisdom\n",
      "\n",
      "Model 1 :  Wisdom the band america all the very worst friends people picture the go is all do nfl of is there time that is all communication make.\n",
      "Model 2 :  Wisdom is the truth that all the time is the supreme source of the appeal of the error to the truth and the truth is not.\n",
      "------------------------------------------------------------------------------------------------------\n",
      "Randomly chosen starting word:  friendship\n",
      "\n",
      "Model 1 :  Friendship on one not a become cool because that more have argue my a earning of talk.\n",
      "Model 2 :  Friendship is a fool to know the truth and another is not a fool to be wise and.\n",
      "------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "while i < 5:\n",
    "    startingWord = chooseStartingWord(wordCategories)\n",
    "    print('Randomly chosen starting word: ',startingWord)\n",
    "    numberOfWords = random.randint(12,25)\n",
    "    startingWord = startingWord.strip()\n",
    "\n",
    "    outputQuote = generate_quote(startingWord, num_words = numberOfWords, model= model1, maxlen=numberOfWords)\n",
    "    filteredQuote = filterQuote(outputQuote, illegalWordsList)\n",
    "    outputQuote2 = generate_quote(startingWord, num_words = numberOfWords, model= model2, maxlen=numberOfWords)\n",
    "    filteredQuote2 = filterQuote(outputQuote2, illegalWordsList)\n",
    "    print(\"Model 1 : \",filteredQuote)\n",
    "    print(\"Model 2 : \",filteredQuote2)\n",
    "    i+= 1\n",
    "    print('------------------------------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
